<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera">
  <meta name="keywords" content="Dynamic Reconstruction, Monocular RGB-D Reconstruction, Neural Implicit Function">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NDR</title>

  <!-- Bootstrap -->
  <link href="static/css/bootstrap-4.4.1.css" rel="stylesheet">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title" style="margin-top: 0; margin-bottom: 0">Neural Surface Reconstruction of Dynamic Scenes</h2>
          <h2 class="title is-2 publication-title" style="margin-top: 0">with Monocular RGB-D Camera</h2>
          <h2 class="title is-4 publication-title" style="color:#6e6e6e;margin-top: 2; margin-bottom: 2">NeurIPS 2022</h2>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://rainbowrui.github.io/">Hongrui Cai</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://github.com/WanquanF">Wanquan Feng</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=5G-2EFcAAAAJ">Xuetao Feng</a><sup>2</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              Yan Wang<sup>2</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Alibaba Group</span>&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2206.15258.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!--
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/USTC3DV/NDR-code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Poster Link. -->
              <span class="link-block">
                <a href="./resources/NDR_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!--
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./resources/teaser.png"
                type="video/mp4">
      </video>
      -->
      <img src="./resources/teaser.png" class="center">
      <h2 class="subtitle has-text-centered" style="margin-top: 15px">
        TL;DR: We propose Neural-DynamicReconstruction (NDR), a template-free method to recover high-fidelity geometry (shown in right), motions and appearance (shown in left) of a dynamic scene from a monocular RGB-D camera.
      </h2>
    </div>
  </div>
</section>

<!--
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reconstructions</h2>

        <div class="embed-responsive embed-responsive-16by9">

          <iframe style="clip-path: inset(1px 1px)" src="https://sketchfab.com/playlists/embed?collection=abee3cc1a7a7436c804f2bd3aadc2acd" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true" width="100%" height="100%" frameborder="0"></iframe>
        </div>
        

      </div>
    </div>

  </div>
</section>

-->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose Neural-DynamicReconstruction (NDR), a template-free method to recover high-fidelity geometry and motions of a dynamic scene from a monocular RGB-D camera.
            In NDR, we adopt the neural implicit function for surface representation and rendering such that the captured color and depth can be fully utilized to jointly optimize the surface and deformations.
            To represent and constrain the non-rigid deformations, we propose a novel neural invertible deforming network such that the cycle consistency between arbitrary two frames is automatically satisfied.
            Considering that the surface topology of dynamic scene might change over time, we employ a topology-aware strategy to construct the topology-variant correspondence for the fused frames.
            NDR also further refines the camera poses in a global optimization manner.
            Experiments on public datasets and our collected dataset demonstrate that NDR outperforms existing monocular dynamic reconstruction methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Method</h2>

        <img src="./resources/pipeline.png" class="center">
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            We adopt the neural SDF and radiance field to respectively represent the high-fidelity geometry and appearance in the canonical space (shown in top).
            In our framework, each RGB-D frame can be integrated into the canonical representation.
            We propose a novel neural deformation representation (shown in bottom) that implies a continuous bijective map between observation and canonical space.
            The designed invertible module applies a cycle consistency constraint through the whole RGB-D video; meanwhile, it fits the natural properties of non-rigid motion well.
          </p>
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Results</h2>

        <div class="content has-text-justified">
          <p>
            We test our NDR on multiple datasets (KillingFusion dataset [Slavcheva et al. 2017], DeepDeform dataset [Bozic et al. 2020] and our collected dataset) which covers various object classes and challenging cases.
          </p>
        </div>
        
        <h3 class="title is-4">Human Body and Head</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/body_head.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Plant, Toy and Cloth</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/plant_toy_cloth.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Reconstruction Showcase</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/reconstruction.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Comparisons. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Comparisons</h2>

        <div class="content has-text-justified">
          <p>
            We compare our NDR with RGB-D based methods and recent RGB based method.
          </p>
        </div>
        
        <h3 class="title is-4">Comparisons with RGB-D Based Methods</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/rgbd_compare.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/tum_compare.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Comparisons with RGB Based Method</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/rgb_compare.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Ablation Studies. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Ablation Studies</h2>

        <div class="content has-text-justified">
          <p>
            We also evaluate several key components of our NDR regarding their effects on the final reconstruction result.
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/ablationstudy.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    
    <div class="content has-text-justified">
      <p>
        If you find NDR useful for your work please cite:
      </p>
    </div>

    <pre><code>@article{cai2022neural,
  title     = {Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera},
  author    = {Cai, Hongrui and Feng, Wanquan and Feng, Xuetao and Wang, Yan and Zhang, Juyong},
  journal   = {Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2022},
}</code></pre>
  </div>
</section>



<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    This research was partially supported by the National Natural Science Foundation of China (No.62122071, No.62272433),
    the Fundamental Research Funds for the Central Universities (No. WK3470000021), and Alibaba Group through Alibaba Innovation Research Program (AIR).
    The opinions, findings, conclusions, and recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies or the government.
    We thank the authors of OcclusionFussion for sharing the fusion results of several RGB-D sequences.
    We also thank the authors of BANMo for their suggestions on experimental parameter settings.
    Special thanks to Prof. Weiwei Xu for providing some help.
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
</footer>

</body>
</html>
