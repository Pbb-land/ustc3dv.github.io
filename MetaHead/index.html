<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MetaHead: An Engine to Create Realistic Digital Head">
  <meta name="keywords" content="3D Head Reconstruction, 3D Head Generation, 3D Head Control, Top-Down Customizable Labeled Head Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MetaHead</title>

  <!-- Bootstrap -->
  <link href="static/css/bootstrap-4.4.1.css" rel="stylesheet">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="jumbotron jumbotron-fluid">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <div class="hero-body">
            <div class="container is-max-desktop">
              <div class="columns is-centered">
                <div class="column has-text-centered">
                  <h1 class="title is-2 publication-title" style="margin-top: 0; margin-bottom: 0">MetaHead: An Engine to Create Realistic Digital Head</h1>
                  <br>
                  <!-- <h2 class="title is-2 publication-title" style="margin-top: 0">with Monocular RGB-D Camera</h2> -->
                  <!-- <h2 class="title is-4 publication-title" style="color:#6e6e6e;margin-top: 2; margin-bottom: 2">NeurIPS 2022 (Spotlight)</h2> -->

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      Dingyun Zhang<sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
                    <span class="author-block">
                      Chenglai Zhong<sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
                    <span class="author-block">
                      <a href="https://yudongguo.github.io/">Yudong Guo</a><sup>2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
                      <span class="author-block">
                        <a href="https://hy1995.top/">Yang Hong</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
                    <span class="author-block">
                      <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a><sup>1</sup>
                    </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Science and Technology of China</span>&nbsp;&nbsp;&nbsp;&nbsp;
                    <span class="author-block"><sup>2</sup>Image Derivative Inc</span>&nbsp;&nbsp;&nbsp;&nbsp;
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a class="external-link button is-normal is-rounded is-dark disabled">
                          <span class="icon">
                              <i class="ai ai-arxiv"></i>
                          </span>
                          <span>Paper(Coming Soon)</span>
                        </a>
                      </span>
                      <!-- PDF Link. -->
                      <!--
                      <span class="link-block">
                        <a href=""
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>
                      -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/2206.15258.pdf"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="ai ai-arxiv fa-lg"></i>
                          </span>
                          <span>arXiv</span>
                        </a>
                      </span> -->
                      <!-- Video Link. -->
                      <!--
                      <span class="link-block">
                        <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-youtube"></i>
                          </span>
                          <span>Video</span>
                        </a>
                      </span>
                      -->
                      <!-- Code Link. -->
                      <!-- <span class="link-block">
                        <a href="https://github.com/USTC3DV/NDR-code"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-github fa-lg"></i>
                          </span>
                          <span>Code</span>
                          </a>
                      </span> -->
                      <!-- OpenReview Link. -->
                      <!-- <span class="link-block">
                        <a href="https://openreview.net/forum?id=8RKJj1YDBJT"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fas fa-users"></i>
                          </span>
                          <span>OpenReview</span>
                          </a>
                      </span> -->
                      <!-- Poster Link. -->
                      <!-- <span class="link-block">
                        <a href="https://rainbowrui.github.io/data/NDR_poster.pdf"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-image"></i>
                          </span>
                          <span>Poster</span>
                          </a>
                      </span> -->
                      <!-- Dataset Link. -->
                      <!--
                      <span class="link-block">
                        <a href="https://github.com/google/nerfies/releases/tag/0.1"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="far fa-images"></i>
                          </span>
                          <span>Data</span>
                          </a>
                      </span>
                      -->
                    </div>

                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>  
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="replay-video"
             autoplay
             controls
             muted
             preload
             playsinline
             loop
             width="100%">
        <source src="Introduction.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section> -->
  
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
<!--       <div class="column is-four-fifths"> -->
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Collecting and labeling training data is one important step for learning-based methods because the process is time-consuming and biased. For face analysis tasks, although some generative models can be used to generate face data, they can only achieve
            a subset of generation diversity, reconstruction accuracy, 3D consistency, high-fidelity visual quality, and easy editability. One recent related work is the graphics-based generative method, but it can only render low realism head with high computation cost. 
            In this paper, <b>we propose MetaHead, a unified and full-featured controllable digital head engine</b>, which consists of a controllable head radiance field(MetaHead-F) to super-realistically generate or reconstruct view-consistent 3D controllable digital heads
            and a generic top-down image generation framework LabelHead to generate digital heads consistent with the given customizable feature labels. Experiments validate that our controllable digital head engine achieves the state-of-theart generation visual quality and reconstruction accuracy.
            Moreover, the generated labeled data can assist real training data and significantly surpass the labeled data generated by graphics-based methods in terms of training effect.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Approach. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Approach</h2>

        <img src="./project_page_assets/pipeline.png" class="center">
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            Overview of digital head engine MetaHead. It consists of a controllable head radiance field (MetaHead-F) to super-realistically
            reconstruct or generate view-consistent 3D controllable digital heads and a generic top-down image generation framework LabelHead to
            generate heads consistent with the given customizable feature labels. Furthermore, it can also bottom-up estimate the labels of head features bidirectionally.
          </p>
          <video id="replay-video"
             autoplay
             controls
             preload
             playsinline
             loop
             width="100%">
          <source src="./project_page_assets/Introduction.mp4"
                type="video/mp4">
          </video>
          <hr style="margin-top:0px">
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- MetaHead-F. -->
    <div class="content has-text-justified">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">I. MetaHead-F: Super-realistically Reconstruct or Generate View-consistent 3D Controllable Digital Heads</h2>
      </div> 
      <div class="column is-full-width">
<!--           <p>
            MetaHead-F provides precise and view-consistent 3D control over the shape and appearance of heads in a highly decoupled manner, including challenging expressions and illumination. In comparison, previous models could only output or control moderate expressions.
          </p> -->
        </div>
        <h3 class="title is-4">Common Attributes Control Results</h3>
        <div class="content has-text-justified">
          <p>
            MetaHead-F provides precise and view-consistent 3D control over the shape and appearance of heads in a highly decoupled manner, including challenging expressions and illumination. In comparison, previous models could only output or control moderate expressions. It can also generate heads in a 3D-aware stylization manner.
          </p>
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./project_page_assets/CommonAttributesControllingResults.mp4"
                    type="video/mp4">
          </video>
          <br>
          <img src="./project_page_assets/CommonAttributesControllingResults.png" class="center">
          <hr style="margin-top:0px">
        </div>
        
        <br>
        <h3 class="title is-4">Super-resolution & Hierarchical Structural Attention Module</h3>
        <div class="content has-text-justified">
          <p>
            Super-resolution module and our proposed hierarchical structure attention module solve the chronic dynamic-scene problem of hair, teeth and beard flickering when the viewing angle changes.
          </p>
          <br>
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./project_page_assets/DynamicSceneProblem.mp4"
                    type="video/mp4">
          </video>
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./project_page_assets/ThetaComparison.mp4"
                    type="video/mp4">
          </video>
          <p>
            <b>Top:</b> state-of-the-art head models often experience texture flickering when used in dynamic scenes, which sets them apart from real human heads.
            <b>Bottom:</b> video sequences comparing the super-resolution output (left half of each scene) to the neural volume rendering (right half of each scene).
          </p>
          <hr style="margin-top:0px">
        </div>
        
        <br>
        <h3 class="title is-4">Reconstruction Accuracy and 3D View Consistency</h3>
        <p>
          Zero-shot self-retargeting results demonstrate MetaHead-F's precise reconstruction of input human heads, 3D view consistency, and decoupling of pose control.
        </p>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./project_page_assets/SelfRetargeting.mp4"
                    type="video/mp4">
          </video>
          <hr style="margin-top:0px">
        </div>
        
        <br>
        <h3 class="title is-4">Comparison with State-of-the-art Models</h3>
        <p>
          We present qualitative results on single-image reconstruction and attribute control accuracy, comparing our method against four state-of-the-art head synthesis or reconstruction-only methods. In each sub-figure, column 3 displays the reconstruction results, while columns 2 and 4 show the results obtained by controlling the camera poses of the reconstructions. 
<!--           MetaHead-F accurately reconstructs factors covering the shape and appearance of the heads, with precise pose control that is highly view-consistent. During pose editing, only the poses change while the other head properties remain unchanged, demonstrating effective and robust control disentanglement. -->
        </p>
        <div class="content has-text-justified">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./project_page_assets/SingleImageReconstruction.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>
        <p>
          We present qualitative results on the expression control, comparing our method against the state-of-the-art controllable head synthesis method GAN-Control. For each sub-figure, column 1 shows a reference image, columns 2 to 7 show images generated with random expression control. 
<!--           MetaHead-F generates compelling photo-realistic heads with highly consistent, precise expression control. When controlling expressions for the heads in each row, the other head attributes, such as identity and texture, remain unchanged. Besides,  -->
          MetaHead-F allows for diverse expression variations, including frowning, pouting, curling lips, smirking, and more.
        </p>
        <div class="content has-text-justified">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./project_page_assets/ExpressionVariation.mp4"
                    type="video/mp4">
          </video>
          <hr style="margin-top:0px">
        </div>
        
        <br>
        <h3 class="title is-4">Validations of Proposed Components</h3>
        <p>
          We show ablation experiments on SemanticField, super-resolution module and our proposed conditional supervision head signals.  Studies show that SemanticField precisely control the hair and mouth shape, our proposed identity and expression conditional prior signal enhance the control over fine-grained geometric details(identity and expression), and super-resolution module significantly improves the output visual quality.
        </p>
        <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./project_page_assets/AblationStudy.mp4"
                    type="video/mp4">
         </video>
         <hr style="margin-top:0px">
<!--         <div class="content has-text-justified">
          <img src="./project_page_assets/Ablation_2.png" class="center">
          <hr style="margin-top:0px">
        </div> -->
      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Comparisons. -->
    <div class="content has-text-justified">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">II. LabelHead: a Generic Top-down Image Generation Framework</h2>
      </div>
      <h3 class="title is-4">Control over Customizable Head Attributes</h3>
      <div class="content has-text-justified">
        <p>
          The feature design space of MetaHead-F has many customizable options in addition to identity, illumination and texture.  We present examples of adding other features to the feature design space, which verifies that LabelHead allows MetaHead-F to precisely control more attributes such as gaze and hair color in addition to the common attributes in a disentanglement manner, which cannot be achieved by existing models.
        </p>
        <video id="replay-video"
               autoplay
               controls
               muted
               preload
               playsinline
               loop
               width="100%">
          <source src="./project_page_assets/GazeAndHairColorControl.mp4"
                  type="video/mp4">
        </video>
        <br>
        <br>
        <p>
          we also present synthesizing heads with consistent semantic labels and the interactive local control over head shape using semantic labels.
        </p>
        <img src="./project_page_assets/HeadShapeControl.png" class="center">
        <hr style="margin-top:0px">
      </div>
      
      <br>
      <h3 class="title is-4">Synthesizing Labeled Training Images</h3>
      <div class="content has-text-justified">
        <p>
          Methods for head label estimation such as landmark and gaze have not yet reached a satisfactory level of performance. This is primarily due to the lack of sufficiently large and diverse labeled training data for the task. <b>Collecting precise and highly varied head labeled data with ground truth, particularly outside of the lab, is a challenging task.</b> In the main manuscript, due to the limited space, we discuss landmark feature in detail. 
<!--           We embed the landmark feature 3D-HeadPoints into the feature design space and fine-tune the pre-trained MetaHead-F on 300W  training images for 2D face alignment task and Face Synthetics for 3D face alignment task, respectively.  -->
          We show 2D landmark labels in (a) row 1-2, 3D landmark labels in (a) row 3 and (b). (a) shows that LabelHead could generate super-realistic heads under challenging scenes and diverse shape and appearance variation with accurate labels. (b) shows that MetaHead-F could synthesize stylized heads with equally accuracy. 
        </p>
        <img src="./project_page_assets/LabeledTrainingImages.png" class="center">
      </div>
      
      <div class="columns is-centered">
        <!-- Landmark Estimation. -->
        <div class="column">
          <div class="content">
            <h4 class="title is-5">Synthesizing Landmark Labeled Training Data</h2>
            <p>
              Our synthesized heads help label estimation with small amount of real data, and surpasses state-of-the-art landmark-labeled image generation method [45] on head synthesis.
            </p>
            <img src="./project_page_assets/LandmarkEstimation.png" class="center">
          </div>
        </div>
        <!--/ Landmark Estimation. -->

        <!-- Gaze Estimation. -->
        <div class="column">
          <h4 class="title is-5">Synthesizing Gaze Labeled Training Data</h2>
          <div class="columns is-centered">
            <div class="column content">
              <p>
                Our synthesized heads help label estimation with small amount of real data on popular gaze benchmarks.
              </p>
              <img src="./project_page_assets/GazeEstimation.png" class="center">
            </div>
          </div>
        </div>
      <!--/ Gaze Estimation. -->
      </div>
      <hr style="margin-top:0px">
  </div>
</section>
  
 
  
<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">III. Application</h2>
      </div>
      <h3 class="title is-4">Text-to-head Generation</h3>
      <div class="content has-text-justified">
        <p style="margin-top: 30px">
          A natural way to customize 3D heads is to use language guidance. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of
          the many degrees of freedom.  We explore leveraging the power of Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for MetaHead-F head generation and
          manipulation that does not require such manual effort. 
<!--           Our simple yet efficient approach for leveraging CLIP to guide image manipulation is to perform the direct latent code optimization.  -->
          As shown in the figure, one can finely customize the super-realistic 3D heads using text descriptions. 
        </p>
<!--         <video id="replay-video"
           autoplay
           controls
           muted
           preload
           playsinline
           loop
           width="100%">
        <source src="./project_page_assets/TextGenerate.mp4"
              type="video/mp4">
        </video> -->
        <img src="./project_page_assets/TextGenerate.png" class="center">
      </div>
      <hr style="margin-top:0px">
      <br>
      <h3 class="title is-4">Text-based 3D Head Manipulation</h3>
      <div class="content has-text-justified">
        <p style="margin-top: 30px">
          We can further semantically 3D edit generated heads using text prompts through solving the optimization problem of minimize the cosine distance between the CLIP embeddings of its text and image inputs. As shown in the figure, we can achieve a wide variety of disentangled and meaningful control faithful to the text prompt. 
        </p>
<!--         <video id="replay-video"
           autoplay
           controls
           muted
           preload
           playsinline
           loop
           width="100%">
        <source src="./project_page_assets/TextEdit.mp4"<img src="./project_page_assets/TextGenerate.png" class="center">
              type="video/mp4">
        </video> -->
        <img src="./project_page_assets/TextEdit.png" class="center">
      </div>

    </div>
  </div>
</section>
      
 
  
  
  
  
  
  
  
  
   



<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    
    <div class="content has-text-justified">
      <p>
        If you find MetaHead useful for your work please cite:
      </p>
    </div>

    <pre><code>@inproceedings{Cai2022NDR,
  author    = {Hongrui Cai and Wanquan Feng and Xuetao Feng and Yan Wang and Juyong Zhang},
  title     = {Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2022}
}</code></pre>
  </div>
</section> -->



<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    This research was partially supported by the National Natural Science Foundation of China (No.62122071, No.62272433),
    the Fundamental Research Funds for the Central Universities (No. WK3470000021), and Alibaba Group through Alibaba Innovation Research Program (AIR).
    The opinions, findings, conclusions, and recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies or the government.
    We thank the authors of OcclusionFussion for sharing the fusion results of several RGB-D sequences.
    We also thank the authors of BANMo for their suggestions on experimental parameter settings.
    Special thanks to Prof. Weiwei Xu for providing some help.
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            The webpage template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and we appreciate <a href="https://keunhong.com/">Keunhong Park</a> for open-sourcing the template.
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
</footer>

</body>
</html>
